\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{todonotes}
\title{Machine Learning Engineer Nanodegree Capstone Project}
\author{Mateusz Bednarski}
\date{\today}


\usepackage{graphicx}
\graphicspath{ {images/} }

\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{hyperref}


\begin{document}

\maketitle



\section{Introduction}

Domain for my capstone project is control problems or robot motion. This requires either a physical robot or simulation engine. For purposes of this project I decided to make use of OpenAI Gym - \url{https://gym.openai.com}. Gym is set of environments simulating various tasks. It provides ready to use simulators and frameworks for comparing algorithms. From available environments, I have selected "LunarLander-v2". For a few reasons:
\begin{itemize}
\item For first, reinforcement learning interested me the most, so I want to get deeper into this.
\item OpenAI provides leaderborads for each environement, making result comparison easy
\item OpenAI provides problem implementations, thus I can focus only on RL part.
\item I love space and landing on a planet/moon is interesting problem for me.
\item It has (approximately – more details in dataset section) continuous state space, so basic tabular Q-learning will not work – I need to examine more sophisticated techniques
\item Lunar lander is quite challenging environment
\end{itemize}


Let's briefly describe both selected problem.

\section{Problem Statement}

There are a landing pad and a lander. Lander has two legs, and three engines – thrust directed down, left and right. Possible actions are one of do nothing/fire main engine/fire left engine/fire right engine.  Task is to land on the landing pad.  Lander starts above the pad and is affected by gravity. Simulation is finished either lander lands or crashes. Landing outside pad is also possible. Fuel is unlimited and there is no time penalty.
Rewards and penalties are already defined in the environment:
\begin{itemize}
\item Moving towards pad with zero speed: from +100 to +140
\item Crash: -100
\item Successful landing: +100
\item Leg with ground contact: +10
\item Firing main engine (not side engine): -0.3
\item When landing outside pad, an additional penalty is given.
\end{itemize}

Environment is considered as solved, when average episode for 100 consecutive episodes is at least 200.

\subsection{Evaluation metric}

For measuring performance I will use moving average of cumulative reward for last 100 episodes, and number of iterations to reach it.

\section{Analysis}
\subsection{Data Exploration}

As it is a reinforcement learning problem, there is no dataset understood in classical way. Instead, I explored mechanics of problems more deeply. Data about states distribution for was generated using random action selection. There is a need to be careful with this data - for random walking probably, many states will not be visited much often. However it provides an overview.

The same data was grabbed for solving agent to compare distributions.

\subsubsection{Action and state space}

Action space is a discrete, finite set:

\begin{equation}
\begin{aligned}
&A = {0,1,2,3} \\
&|A| = 4
\end{aligned}
\end{equation}

State space is a vector of 8 real numbers describing lander position, velocity and orientation. Last two components says if left and right lander leg touch ground. If so, they are set to 1, instead - 0.

\begin{equation}
\begin{aligned}
&S \in \mathbb{R}^8 \\
&s_{0..5} \in (-\infty, +\infty) \\
&s_{6,7} \in \{0,1\} \\
\end{aligned}
\end{equation}

\subsection{Space size}

Before, I said that space consists of real numbers. Given that, space state size  would be uncountably infinite. However, during to machine representation of real numbers it is not exactly true. In typical implementation, \emph{float} can handle one of $2^{52}$ values \footnote{\url{http://stackoverflow.com/a/8875223}}. Storing Q-table for LunarLander would take:

\begin{equation}
|Q| = |S \times A| = |S| \times |A| = \underbrace{{(2^{52})}^6 \cdot 2^2}_{|S|} \cdot \underbrace{4}_{|A|} \approx 2.24 \cdot 10^{102}
\end{equation}

Which requires $9 \cdot 10^{89}$ PB of memory. Also, each state should be visited enough number of times. It definetely makes this unsolvable by tabular Q-learning.

\subsection{Algorithms}

Because Q-space ($S \times A$) is infinite in size, it is impossible to use Q lookup table. There are many approaches to deal with this problem. There are at least two approaches to deal wit this problem. First, is to replace Q table with a function $h(s,a) \rightarrow \mathbb{R}$, that will approximate $Q(s, a)$ values. Therefore, the goal is to find a function $h(s, a)$, that will return a Q-value for a given pair $(s,a)$. Another one, is to discretize continuous space. Hovewer, I decided to use completely different approach.

\subsection{Parameter-exploring Policy Gradient}
Instead of using techniques that are focused on searching function approximating qvalues, PEPG uses \emph{policy space}. Ultimate goal is to find a policy that will solve the environment. This algorithms search  the policy space in order to find such one. \todo{source}

PEPG is a policy gradient method, it does mean that it estimates policy gradient and use it to maximize rewards given by following a policy.

\subsubsection{Policy}

For first, we define a parametrized policy $\pi_\theta$

\begin{equation}
\pi_\theta: s \rightarrow a
\end{equation}

For this algorithm purposes, the policy can be basically free-form. I used three layered feed-forward neural network. (two hidden layers, and one output layer).
Both hidden layers contains 32 neurons each. Output layer has 4 neurons - one per possible action. All layers has $\tanh$ activation function.

If we denote $x$ as state, $\theta = (W,b,W_2,b_2,W_3, b_3)$ the whole policy is following:

\begin{equation}
\begin{aligned}
z &= xW + b \\
\varphi &= \tanh(z) \\
z_2 &= \varphi W_2 + b_2 \\
\varphi_2 &= \tanh(z_2) \\
z_3 &= \varphi W_3 + b_3 \\
\varphi_3 &= \tanh(z_3) \\
\text{selected action} &= \argmax(\varphi_3)
\end{aligned}
\end{equation}

There is no need to backpropagate this network.

\subsubsection{Definitions}

\begin{itemize}

\item $\theta$ is policy parametrization (one-dimensional vector)

\item $a_t = \pi_\theta(s_t)$ is action taken in step $t$ by following policy $\pi_\theta$

\item $s_t$ is state in step $t$

\item $r_t$ is reward received in step $t$

\item $h = [s_{1:T}, a_{1:T}]$ is a complete history of episode from the beginning to the last step $T$
\item $R(T) = \sum_{t=1}^{T} r_t$ is sum of rewards during episode $h$

\item $\mathcal{N}(\mu, \sigma^2)$ is normal distribution with mean $\mu$ and variance $\sigma^2$

\item $N$ is number of histories
\item $P$ in size of parametrization $\theta$. $\theta \in \mathbb{R}^P$

\item $\alpha, \alpha_\mu, \alpha_\sigma$ are various learning rates

\item $\mu$ is current mean parametrization
\item $\sigma$ is current standard deviation of parametrization

\item $T$ and $S$ are matrices with shape $P\times N$

\item $b$ is mean reward for last 50 iterations

\end{itemize}


\subsection{Benchmark}

Select result to compare.

\section{Methodology}
\subsection{Data Preprocessing}
\subsubsection{Linear Model}
\subsubsection{State space discretization}

SARSA kNN assumes that states are in range $[-1;1]$ but as shown in section Data Exploration space state for both problems does not statisfy this assumption. In order to transform space state, following algorithm was used. Selected action is one, that has highest output on corresponding neuron. 

\begin{equation}
\begin{aligned}
&i={0,1,\ldots, d-1} \\
&x_i = \frac{s_i - \min(s_1)}{\max(s_i) - \min(s_i)} - 1
\end{aligned}
\end{equation}

Where $d$ is dimensionality of original space state, $s_i$ is an element of a state vector, $x_i$ is the same element preprocessed.

This must be done before running agent. For Mountain Car bounds for states are well-defined. For Cart pole they are not. In order to find max and min, from visited states was collected and algorithm was re-launched with updated min and max.

\end{document}


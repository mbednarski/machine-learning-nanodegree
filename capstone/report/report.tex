\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{todonotes}
\title{Machine Learning Engineer Nanodegree Capstone Project}
\author{Mateusz Bednarski}
\date{\today}


\usepackage{graphicx}
\graphicspath{ {images/} }

\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{hyperref}


\begin{document}

\maketitle



\section{Introduction}

Domain for my capstone project is control problems or robot motion. This requires either a physical robot or simulation engine. For purposes of this project I decided to make use of OpenAI Gym - \url{https://gym.openai.com}. Gym is set of environments simulating various tasks. It provides ready to use simulators and frameworks for comparing algorithms. From available environments, I have selected "LunarLander-v2". For a few reasons:
\begin{itemize}
\item For first, reinforcement learning interested me the most, so I want to get deeper into this.
\item OpenAI provides leaderborads for each environement, making result comparison easy
\item OpenAI provides problem implementations, thus I can focus only on RL part.
\item I love space and landing on a planet/moon is interesting problem for me.
\item It has (approximately – more details in dataset section) continuous state space, so basic tabular Q-learning will not work – I need to examine more sophisticated techniques
\item Lunar lander is quite challenging environment
\end{itemize}


Let's briefly describe both selected problem.

\section{Problem Statement}

There are a landing pad and a lander. Lander has two legs, and three engines – thrust directed down, left and right. Possible actions are one of do nothing/fire main engine/fire left engine/fire right engine.  Task is to land on the landing pad.  Lander starts above the pad and is affected by gravity. Simulation is finished either lander lands or crashes. Landing outside pad is also possible. Fuel is unlimited and there is no time penalty.
Rewards and penalties are already defined in the environment:
\begin{itemize}
\item Moving towards pad with zero speed: from +100 to +140
\item Crash: -100
\item Successful landing: +100
\item Leg with ground contact: +10
\item Firing main engine (not side engine): -0.3
\item When landing outside pad, an additional penalty is given.
\end{itemize}

Environment is considered as solved, when average episode for 100 consecutive episodes is at least 200.

\subsection{Evaluation metric}

For measuring performance I will use moving average of cumulative reward for last 100 episodes, and number of iterations to reach it.

\section{Analysis}
\subsection{Data Exploration}

As it is a reinforcement learning problem, there is no dataset understood in classical way. Instead, I explored mechanics of problems more deeply. Data about states distribution for was generated using random action selection. There is a need to be careful with this data - for random walking probably, many states will not be visited much often. However it provides an overview.

The same data was grabbed for solving agent to compare distributions.

\subsubsection{Action and state space}

Action space is a discrete, finite set:

\begin{equation}
\begin{aligned}
&A = {0,1,2,3} \\
&|A| = 4
\end{aligned}
\end{equation}

State space is a vector of 8 real numbers describing lander position, velocity and orientation. Last two components says if left and right lander leg touch ground. If so, they are set to 1, instead - 0.

\begin{equation}
\begin{aligned}
&S \in \mathbb{R}^8 \\
&s_{0..5} \in (-\infty, +\infty) \\
&s_{6,7} \in \{0,1\} \\
\end{aligned}
\end{equation}

\subsection{Space size}

Before, I said that space consists of real numbers. Given that, space state size  would be uncountably infinite. However, during to machine representation of real numbers it is not exactly true. In typical implementation, \emph{float} can handle one of $2^{52}$ values \footnote{\url{http://stackoverflow.com/a/8875223}}. Storing Q-table for LunarLander would take:

\begin{equation}
|Q| = |S \times A| = |S| \times |A| = \underbrace{{(2^{52})}^6 \cdot 2^2}_{|S|} \cdot \underbrace{4}_{|A|} \approx 2.24 \cdot 10^{102}
\end{equation}

Which requires $9 \cdot 10^{89}$ PB of memory. Also, each state should be visited enough number of times. It definetely makes this unsolvable by tabular Q-learning.

\subsection{Algorithms}

Because Q-space ($S \times A$) is infinite in size, it is impossible to use Q lookup table. There are at least two approaches to deal wit this problem. First, is to replace Q table with a function $h(s,a) \rightarrow \mathbb{R}$, that will approximate $Q(s, a)$ values. Therefore, the goal is to find a function $h(s, a)$, that will return a Q-value for a given pair $(s,a)$. The second one, is to discretize continuous space. Both of them were implemented.
\\[12pt]
This approach allows to:
\begin{enumerate}
\item Handle continuous Q-space
\item Generalize knowledge to unvisited states
\item Provides robustness – similar Q-states will have similar Q-values
\end{enumerate}

\subsection{Linear model}
The most basic model will be to use linear function:

\begin{equation}
h(s, a) = \sum_i^n f_i(s, a)w_i
\end{equation}

There is a need to describe $f_i$. As we can see, $h(s,a)$ is a linear combination of $n$ $f_i$ functions. Each $f_i$ function can represent a different feature. Feature in the most simple cases, can be just observations. For example, if Q-space is defined as 

\begin{equation}
\begin{aligned}
&S = \mathbb{R}^4 \\
&A = \{0, 1\} \\
&Q = S \times A
\end{aligned}
\end{equation}

we could create following $f_i$ functions

\begin{equation}
\begin{aligned}
&f_0(s, a) = 1 \\
&f_1(s, a) = s_1 \\
&f_2(s, a) = s_2 \\
&f_3(s, a) = s_3 \\
&f_4(s, a) = s_4 \\
&f_5(s, a) = a \\
\end{aligned}
\end{equation}


Phase of creating $f_i$ functions is called \emph{feature engineering}. Used features are described in detail in section data preprocessing.

Therefore, update rule will be following:
\begin{equation}
\begin{aligned}
\delta&=(R(s,a,s')+\gamma V(s',a'))-Q(s,a) \\
w_i &\leftarrow w_i+\alpha \delta f_i (s,a)
\end{aligned}
\end{equation}

Where V is defined as:
\begin{equation}
V(s',a' )=\max_{a'}Q(s',a')
\end{equation}

After each step, we are updating weights in function approximating q-values.

Another possibility is to use 2 separate $Q(s,a)$ functions - one per action. In our example that would be separate $Q_a$ for each action:

\begin{equation}
\begin{aligned}
&Q(s, a) = Q_a(s) \\
&Q_1(s) = \sum_i^n f_i(s)w_{1i} \\
&Q_2(s) = \sum_i^n f_i(s)w_{2i} \\
\end{aligned}
\end{equation}

This can be useful because each action have a separate model and they will not interact with each other. I suppose linear model might be not sufficient to handle relations between all possible actions. From the other hand, non-linear model will be more prone to overfitting. However, there is possibility to use engineered features in order to make linear model perform better. This was implemented in project. More details in preprocessing section. 

\subsection{Space discretization using kNN and SARSA}
Another approach is to discretize space state. As we cannot store all continuous values we can discriteze the space. This will reduce size to finite, and allow us to use eg. tabular Q-learning. However, for this problem I used SARSA (Which is on-policy, in contrast of off-policy qlearning - I'll explain difference later) with kNN.

\subsubsection{Discretization}

First step for kNN-SARSA is to normalize input space. I explain this in preprocessing section. At this point, let's assume that state space is normalized into d-dimensional vector of real values in range $(-1, 1)$.

For each dimmension, we place $p$ \emph{nodes}, arranged unformly \footnote{Also other arrange schemes are possible e.g. Gaussian}. Therefore, for $n$ dimensional space, with $p$ nodes and $|A|$ actions we have Q-space with size:

\begin{equation}
|Q| = p^d \cdot |A|
\end{equation}

\subsubsection{Acquiring Q-value for a q-state}

We introduce parameter $k$ just as in normal kNN algorithm witch tells how meany nearest neighbors we consider. For observed state $s$ we find $k$ nearest nodes (using euclidean distance \footnote{Why we normalize}) and for each of them we compute its influence on our observed state. $n$ menas a neighbor node.

\begin{equation}
\forall knn, \quad knn_i = \frac{1}{1+\text{distance}(knn, s)^2}
\end{equation}

Given that, we transform $knn_i$ values in a way to make them sum up to $1$.

\begin{equation}
\forall knn, \quad w_i = \frac{knn_i}{\sum knn_i}
\end{equation}

The $w$ values, becomes weights how much each neighbor node is affecting observed state. Given that q-value for action $a$ in observed state $s$ is:

\begin{equation}
V(a, s) = \sum_{knn} Q(knn, a)w_i
\end{equation}

\subsubsection{Choosing action}

Action to perform is selected by computing $V(a, s)$ for each action $a$ in observed state $s$ and choosing one wiht highest value.

\begin{equation}
\argmax_a V(a, s)
\end{equation}

There is no $\epsilon$ because for exploration-exploitation I use optimistic initial Q-values.

\subsubsection{On-policy learning}

SARSA is a on-policy method. It does mean that it updates q-values using value of $s'$ and current policy '$a''$. Estimations are done assuming current policy will be followed.

\subsubsection{Eligibility traces}

For this algorithm el

\subsubsection{Learning}

\subsection{Benchmark}

Select result to compare.

\section{Methodology}
\subsection{Data Preprocessing}
\subsubsection{Linear Model}
\subsubsection{State space discretization}

SARSA kNN assumes that states are in range $[-1;1]$ but as shown in section Data Exploration space state for both problems does not statisfy this assumption. In order to transform space state, following algorithm was used.

\begin{equation}
\begin{aligned}
&i={0,1,\ldots, d-1} \\
&x_i = \frac{s_i - \min(s_1)}{\max(s_i) - \min(s_i)} - 1
\end{aligned}
\end{equation}

Where $d$ is dimensionality of original space state, $s_i$ is an element of a state vector, $x_i$ is the same element preprocessed.

This must be done before running agent. For Mountain Car bounds for states are well-defined. For Cart pole they are not. In order to find max and min, from visited states was collected and algorithm was re-launched with updated min and max.

\end{document}


\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{gensymb}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{amssymb}
\title{Machine Learning Engineer Nanodegree Capstone Project}
\author{Mateusz Bednarski}
\date{\today}


\usepackage{graphicx}
\graphicspath{ {images/} }

\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{hyperref}


\begin{document}

\maketitle



\section{Introduction}

Domain for my capstone project is solving classical control problems. I have selected two: Cart pole and Mountain car. Both of them are well-know problems in computer science study. For purposes of this project I decided to make use of OpenAI Gym (LINK). Gym is set of environments simulating various tasks. It provides ready to use simulators and frameworks for comparing algorithms. Also it does have environments with Cart Pole\footnote{\url{https://gym.openai.com/envs/CartPole-v1}} and Mountain Car\footnote{\url{https://gym.openai.com/envs/MountainCar-v0}} which are well-know reinforcement-learning problems. There is a few reasons I have chosen this setup:

\begin{itemize}
\item For first, reinforcement learning interested me the most, so I want to get deeper into this.

\item Both selected problems are well-know and used to compare algorithms perfomance
\item OpenAI provides leaderborads for each environement, making result comparison easy
\item 
OpenAI provides problem implementations, thus I can focus only on RL part.
\item Both problems have continous state space, so basic tabular Q-learing will not work. I need to examine more sophisticated techniques
\item I want to solve two problems instead of one, in order to see how solution will generalize (not being strongly problem-specified)
\end{itemize}


Let's briefly describe both selected problems.

\section{Problem Statement}
\subsection{Cart Pole}

There is a frictionless track, and a vehicle attached to it. Vehicle can move left or right. On top of it, pole is attached. Cart cannot stay at place. Goal is to keep pole vertical and not allow it to fall or run out of track by moving cart left/right. Environment already provides reward (only one): +1 every step that pole is upright

Simulation ends either when pole is deviated over $15\degree$ from vertical or cart is 2.4 units away from the center of the track.

\begin{figure}[h]
\includegraphics[width=\textwidth]{cartpole_intro.png} 
\centering
\caption{The Cart Pole environment from OpenAI.}
\end{figure}


\subsection{Mountain Car}
There is a track between two mountains. Vehicle starts in a valley between them. The goal is to climb right top. Car does not have enough power to do this just riding right - it needs to build momentum. 

The only reward of -1 is given every timestep. There is no reward on approach top. But this is sufficent to minimize time spent, as simulation ends when reached right mountain.

\begin{figure}[h]
\includegraphics[width=\textwidth]{mountaincar_intro.png} 
\centering
\caption{The Mountain Car environment form OpenAI.}
\end{figure}


\subsection{Evaluation metric}

For measuring perfomance I will use moving average of cumulative reward of last 100 episodes.

\section{Data Exploration}

As it is a reinforcement learning problem, there is no dataset understood in classical way. Instead, I explored mechanics of probles more deeply. For each problem, data about states distribution for was generated using random action selection. There is a need to be careful with this data - for random walking probably, many states will not be visited much often. Hovewer it provides an overwiew.

\subsection{Cart Pole}

Action space is a discrite, finite set $A = \{0,1\}$ where 0 means go left, and 1 go right.
Space state is a vector of four real values. 
\begin{multline}
 S = (s_0, s_1, s_2, s_3) \in \mathbb{R}^4 \\
-4.8 < s_0 < 4.8 \\
-\infty < s_1 < \infty \\
-0.42 < s_2 < 0.42 \\
-\infty < s_3 < \infty \\
\end{multline}

Meaning is following: $s_0$ - cart position, $s_1$ cart velocity, $s_2$ - pole angle and $s_4$ - pola angular velocity.

\begin{figure}[h]
\includegraphics[width=\textwidth]{exploratory_cartpole.png} 
\centering
\caption{Cart Pole distribution of $S$ for random agent is somewhere around normal. Simulation runned for 100 episodes.}
\end{figure}

\subsection{Mountain Car}
Action space is a discrite, finite set $A = \{0,1,2\}$ wchich means consecutively move accelerate left, do nothing, accelerate right. Space state is a 2-dimensional vector.

\begin{multline*}
 S = (s_0, s_1) \in \mathbb{R}^2 \\
-1.2 < s_0 < 0.6 \\
-0.07 < s_1 < 0.07
\end{multline*}

$s_0$ is position and $s_1$ is velocity.



\begin{figure}[h]
\includegraphics[width=\textwidth]{exploratory_mountaincar.png} 
\centering
\caption{Mountain Car distribution of $S$ for random agent. Simulation runned for 40000 steps.}
\end{figure}

\subsection{Space size}

Before, I said that space consists of real numbers. Given that, space state size (for both problems) would be uncountably infinite. However, during to machine representation of real numbers it is not exactly true. In typical implementation, \emph{float} can handle one of $2^{52}$ values \footnote{\url{http://stackoverflow.com/a/8875223}}. Storing Q-table for 2 actions and one real-valued state would take:



\begin{equation}
|Q| = |S \times A| = |S| \times |A| = 2^{52} \cdot 2 = 2^{53} = 9007199254740992
\end{equation}

Which requires 36 PB of memory. Only for that simple problem. It definetely makes this unsolvable by tabular Q-learning.

\section{Algorithms}

Because Q-space ($S \times A$) is infinite in size, it is impossible to use Q lookup table. The idea is to replace Q table with a function $h(s,a) \rightarrow \mathbb{R}$, that will approximate $Q(s, a)$ values. This will be a model for this problem. 

Therefore, the goal is to find a function $h(s, a)$, that will return a Q-value for a given pair  $(s,a)$. 
\\[12pt]
This approach allows to:
\begin{enumerate}
\item Handle continuous Q-space
\item Generalize knowledge to unvisited states
\item Provides robustness â€“ similar Q-states will have similar Q-values
\end{enumerate}

\subsection{Linear model}
The most basic model will be to use linear function:

\begin{equation}
h(s, a) = \sum_i^n f_i(s, a)w_i
\end{equation}

There is a need to describe $f_i$. As we can see, $h(s,a)$ is a linear combination of $n$ $f_i$ functions. Each $f_i$ function can represent a different feature. Feature in the most simple cases, can be just observations. For example, if Q-space is defined as 

\begin{equation}
\begin{aligned}
&S = \mathbb{R}^4 \\
&A = \{0, 1\} \\
&Q = S \times A
\end{aligned}
\end{equation}

we could create following $f_i$ functions

\begin{equation}
\begin{aligned}
&f_0(s, a) = 1 \\
&f_1(s, a) = s_1 \\
&f_2(s, a) = s_2 \\
&f_3(s, a) = s_3 \\
&f_4(s, a) = s_4 \\
&f_5(s, a) = a \\
\end{aligned}
\end{equation}


Phase of creating $f_i$ functions is called \emph{feature engineering}.

Therefore, update rule will be following:
\begin{equation}
\begin{aligned}
\text{err}&=(R(s,a,s')+\gamma V(s',a'))-Q(s,a) \\
w_i &\leftarrow w_i+\alpha [\text{err}] f_i (s,a)
\end{aligned}
\end{equation}

Where V is defined as:
\begin{equation}
V(s',a' )=\max_{a'}Q(s',a')
\end{equation}

After each step, we are updating weights in function approximating values.

Another possibility is to use 2 separate $Q(s,a)$ functions - one per action. In our example that would be separate $Q_a$ for each action:

\begin{equation}
\begin{aligned}
&Q(s, a) = Q_a(s) \\
&Q_1(s) = \sum_i^n f_i(s)w_{1i} \\
&Q_2(s) = \sum_i^n f_i(s)w_{2i} \\
\end{aligned}
\end{equation}

This can be useful because each action have a separate model and they do not interact with each other. I suppose linear model might be not sufficient to handle relations between all possible actions. From the other hand, non-linear model will be more prone to overfitting. 

\subsection{Space discretization using kNN and SARSA}
Another approach is to discretize space state. As we cannot store all continuous values we can discriteze the space. This will reduce size to finite, and allow us to use eg. tabular Q-learning. Howeverw, for this problem I used SARSA (Wchic icn in-policy, in contrast of off-policy qlearning - Ill explain difference later) with kNN.

\subsubsection{Discretization}

First step for kNN-SARSA is to normalize input space. I explain this in senction SECTION NUMBER. At this point, let's assume that state space is normalized into n-dimensional vector of real values in range $(-1, 1)$.

For each dimmension, we place $p$ \emph{nodes}, arranged unformly \footnote{Also other arrang schemes are possible e.g. Gaussian}. Therefore, for $n$ dimensional space, with $p$ nodes and $|A|$ actions we have Q-space with size:

\begin{equation}
|Q| = n^d \cdot |A|
\end{equation}

\subsubsection{Acquiring Q-value for a point}

We introduce parameter $k$ just as in normal kNN algoruithm wchit tells how meany nearest neighbpurs we consider. For observed state $s$ we find $k$ nearest nodes (using euclidean distance \footnote{Why we normalize}) and for each of them we compute its infuence on our observed state. $n$ meas a neighbour node.

\begin{equation}
\forall knn, \quad knn_i = \frac{1}{1+\text{distance}(knn, s)^2}
\end{equation}

Given that, we transform $knn_i$ values in a way to make them sum up to $1$.

\begin{equation}
\forall knn, \quad w_i = \frac{knn_i}{\sum knn_i}
\end{equation}

The $w$ values, becomes weights how much each neighbour node is affecting observed state. Given that q-value for action $a$ in observed state $s$ is:

\begin{equation}
V(a, s) = \sum_{knn} Q(knn, a)w_i
\end{equation}

\subsubsection{Choosing action}

Action to perform is selected by computing $V(a, s)$ for each action $a$ in observed state $s$ and choosing one wiht highest value.

\begin{equation}
\argmax_a V(a, s)
\end{equation}

There is no $\epsilon$ because for exploration-exploitation I use optimistic inital Q-values.

\subsubsection{Learning}

\section{Benchmark}
dadada

\end{document}

